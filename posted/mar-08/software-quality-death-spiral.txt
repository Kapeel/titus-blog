The (Lack of) Testing Death Spiral
# tags python,testing,pycon

At PyCon '08, I gave a talk on `testing and the OLPC project
<http://us.pycon.org/2008/conference/schedule/event/83/>`__ where I
referred to the "Testing Death Spiral".  My accompanying slide, which
aimed to be simple rather than comprehensive, had this scenario:

1. Write a bunch of code & manually test it.
   
   (Good so far.)

2. Start adding features over *here*.

3. Watch code break over *there*.

4. Rinse, lather, repeat

   (Where do *you* think this ends?)

----

OK, so that format doesn't really work in a blog post, but hopefully
you get the gist of the scenario.  This is a scenario I see a lot: a
project gets hacked together & works well enough that people start
using it; then the project starts to expand. Many new features are
added.  However, as these new (and presumably solid) features are
being added, the old code becomes increasingly ignored, uncovered by
manual testing, and fragile.

This is a simple consequence of an inescapable fact: the amount of
testing needed to detect `regressions
<http://www.answers.com/regressions>`__ scales with the number of
features.  Forget about finding *new* bugs in the code you just wrote
-- I'm talking about breaking *existing* code.

I have seen people attempt to escape this scenario in a number of
ways: improve the architecture and reduce internal linkages; open
source it; release early, release often; alpha- and beta-test it; stop
adding new features; and probably many more.  These are all good
thoughts, but they are all doomed to failure [#]_.  Nonetheless, I
wish you well.

The only solution I have found is this: write automated tests.

Before I continue, let me say: automated tests are not a panacea.
Writing good code is hard, getting your project "out there" is
important, exploratory testing is *mandatory*, and writing appropriate
automated tests is hard; there's a lot more to building software than
writing good, automated tests.  I stress that every time I talk about
test automation.  I just think automated tests are *necessary* [#]_.

Let us suppose, for the sake of argument, that you have some software
that is actively evolving.  Furthermore, this software has no
automated tests.  Every time you add a feature, you test the bejeezus
out of that feature in order to satisfy yourself that it works.  You
do this for every new feature that is added, and thus consider your
software to be solid.

I now have two questions to ask:

 - are you adding features in isolation from each other?  that is, is
   your architecture such that each new features only uses
   non-state-changing code from elsewhere in your project?

   (if the answer is yes -- are you **sure**?)  

 - do you completely control the packages, libraries, compiler,
   operating system, and hardware that your software runs on?

   (if the answer is yes, do you plan to never, ever, change any of
   those components?  and have you discussed these plans with anyone
   outside your development team? and do you *believe* your managers?)

I like to summarize these questions this way: **are you feeling lucky, punk?**

If the answer is "yes" to all of the above, then congratulations --
you are Apple, stuck at one point in time, and never planning to
release a new piece of software or hardware :).  Hopefully you'll do
better than Apple did *before* they decided to change and adapt...

If the answer is "no" to any of the above, I encourage you to read on.

I assert that even with a perfectly decoupled architecture, brilliant
software engineers, and nigh-complete control over the software and
hardware that you use -- in itself a dream software development
situation -- you will eventually need to add features that crosscut
that architecture, and you will also need to upgrade the compiler,
libraries, language version, operating system, and hardware.  In order
to make sure that your software still works, *each time* you add a
feature or change a component, you will have to retest every feature
and every piece of code.  And, if you have no automated tests, you
will have to do this manually.  Every time.

If you have automated tests, however, your development process could
look something like this:

1. change code
2. run tests
3. commit
4. test manually, do exploratory testing
5. find bugs, write automated tests to reduce bugs
6. goto 1

Even if you don't add any new features, this process applies to
library, compiler, platform, and hardware changes.  At the least, you
will be able to quickly determine if you've broken something that
you're testing for; at the best, you will be able to quickly and
confidently release new versions of your software.

.. ugly transition @CTB

Fundamentally, then, automated testing is important for *software
maintenance*.  And since the cost of software maintenance is a
significant portion of the cost of developing the software in the
first place [#]_, it behooves you to pay attention to anything that
will reduce the cost of software maintenance.  This is without even
considering other aspects of test utility, like increased developer
velocity, ease of refactoring, increased confidence in your software,
etc.

This maintenance situation is the scenario that led me into testing
(or, if you prefer, "illuminated me as to the importance of automated
tests by whacking me over the head with a clue bat".)

Let me assure you that this maintenance situation doesn't *just* apply
to large bodies of code, either.  I maintain a number of small
projects and having automated tests means that I simply don't release
code with regressions.  Moreover, when my small projects "grow up"
into bigger ones -- or, more frequently, are used in larger projects
-- I'm not stuck in a situation where I suddenly have to write a bunch
of tests to achieve stability.  I *always* try to grow my test
framework organically with the project, because I will *never* have
the time to put into writing tests from scratch for my bigger
projects.

----

So, automated tests are important for maintenance, and they are
critical for making sure that your old code still works while you
focus on new code.  Without automated tests, you will be doomed to
releasing increasingly buggy software as your body of code increases
and the *average* level of testing decreases.

Does this actually happen?

----

This is precisely the scenario that led to our consulting work with
`ARINC
<http://us.pycon.org/zope/talks/2007/sun/track1/084/talkDetails2>`__,
which went well.  (As in, they're adding new features with great
confidence after we helped them adopt automation tools and practices.)

----

This is also the scenario that leads to what Jamie Zawinski named `the
Cascade of Attention Deficit Teenagers
<http://www.jwz.org/doc/cadt.html>`__.  Open Source projects, facing a
continually increasing number of bugs, often opt to completely rewrite
their components in the expectation that *this* time, they'll get it
right.  This completely ignores our experience with software rewrites,
which suggests that (barring brilliance and luck) any rewrite will
contain as many bugs as the original software -- they'll just be
*different* bugs.  (As JWZ points out, though, it's more fun to write
*new* code than to fix the crud someone else wrote before...)

----

And, finally, it is also the scenario faced by the One Laptop Per
Child project, which has built a tower of cards on open source
software.  Their build system pulls in about fifty distinct packages
*live from the Internet*, compiles them all, and then layers the Sugar
user interface on top of them.

There is no automated testing in place.

----

OK, back to the Software Testing Death Spiral.   What happens to
projects that lack both automated tests and an exponentially increasing
team of testers?  Starting somewhere in the middle of the process:

1. They manually test the new features and bug fixes that they've just added.

2. They release their software.

3. Their software breaks in unexpected locations, bug reports are
   filed, and (optimistically) those bugs are fixed.  Go to #1.

The inevitable consequence is a death spiral, barring only a complete
rewrite (which will possibly fail, or likely lead to a product that's
just as buggy, but with unknown bugs), trashing of the project, OR
-- and this is an optimistic scenario -- the adoption of automated
testing.

----

Here are a few straw men, with moderately snarky replies:

**"We don't test, and we don't use version control.  Which is more
important?"** Version control.  But you're doomed, anyway.

**"We don't have time to test."** Why do you have time to write
software, but not time to make sure it works?

**"We don't have the expertise to build good tests, and/or we can't
afford the tools, and/or we don't know how to use them."** This is a
pretty realistic scenario, actually.  May I suggest: hire consultants,
or read some good books, or dedicate your young new hire to learning
the tools?

**"We don't like to test."** Well, at least you're honest ;).  I would
summarize your choices like this: either you can write crappy
software, or you can learn to like testing.  The former will most
likely doom you to the rubbish bin of history.  The latter gives you a
better chance of "making it".

**"We really do plan to rewrite our software in two years."** Points
for honesty, again!  I think you're rolling the dice -- many software
projects fail, but maybe you'll do better.  Might I suggest an
incremental rewrite rather than a complete rewrite?  (For that you'll
need testing, though...)

**"We wrote a bunch of automated tests.  They didn't help us.** Ahh, a
problem based in actual experience!  I would like to suggest -- with
no background in your particular problem -- that you try out several
different kinds of tests, like functional tests or regression tests,
and see what *does* help you.

**"How do I test, if I don't know what the right answer is, anyway?"**
How do you know you *got* the right answer, then?  If your customers
don't care if you're right, then you've stumbled into a gold mine, but
I daresay it will end badly.  (This straw man was actually sighted at
PyCon -- sorry, MC.)  I hear this a lot in research, actually, but
it's still nonsense.  Perhaps another blog post in there...

**"I can't convince my boss/team leader/PI that it's important to
spend the time to write tests.  (I even sent him/her your blog
post.)"** You could go one of three ways: try harder, integrate
testing into your personal development strategy and view this
situation as an opportunity to "manage up", or quit.  The middle
option is the interesting one: you can quietly start
writing automated tests to "fence in" your own code, and explain to
your boss that this is just how you code -- it's like using emacs
instead of vi -- and you're not insisting that anyone else follow
suit.  Hopefully your productivity will not decrease much, while
your reliability will increase.  Good fellow programmers may follow suit
and at some point your manager might realize that you've all evaded
his dictat.  Or not.  But it beats working on untested code!

**"I am but one lone programmer, and I can't convince my team to
write/use tests.  (I even sent them your blog post.)"** See previous
question/answer: you will find that most worthwhile programmers are in
favor of anything that increases their productivity and reliability.

**"There's so many other things to straighten out on my project before
I can even think about what tests to write."** I sympathize, I really
do, but if your project is so undirected that you can't even figure
out what it's *supposed* to do (and write tests for it) then you have
far bigger problems than bad code to worry about.

**"I took your advice and wrote tests.  Then we changed a bunch of
stuff, and now all the tests break, and I don't have time to fix them.
What do I do now?"** Hmm, this is a common complaint. First, try to
separate out a subset of the tests that are of *immediate* use to you
(as in, they pass and/or they exercise a lot of your code).  Keep that
subset working.  Second, don't be afraid to simply *delete your old
tests*.  Tests should not be a maintenance headache; if you like and
use tests, but don't see the point of maintaining a bunch of your
broken tests, get rid of them!  Then put new ones back in as
necessary.

----

There really are a bunch of other reasons to write automated tests, too.
For example, consider:

 - cross-platform development is dramatically simplified when you have
   a moderately thorough test suite.  In particular, you can develop
   on your favorite machine, in your favorite programming environment,
   and let the continuous integration boxes run and test your code on
   all the other machines.

 - setting up new development environments and development machines is
   much easier when you can simply ... run the tests to figure out if it's
   all working.

 - integrating new people into the development team is much easier when
   they can run tests to figure out if they just broke something.

 - releasing "a quick bugfix" is a lot easier when you can be fairly
   confident that your quick new release is no more broken than your
   last release.

If these aren't enough to make you think seriously about testing, then
I give up!

----

There's no real conclusion to this :).  I'll talk more about the OLPC
stuff later.

Don't get me wrong: testing is *hard*.  Testing effectively is even
harder.  There are ways around this, but the best way to start may be
to simply power through: write a bunch of tests, and ruthlessly
discard those that don't help.  Then refine your method over time.  I
have some advice to offer here, too, but that's for another blog
post...

And remember... `Darth Vader recommends testing!
<http://www.flickr.com/photos/sebastian_bergmann/2282734669/>`__

--titus

p.s. Thanks to Tracy Teal, Lisa Crispin, Alex Gouaillard, Kumar
McMillan, Shannon -jj Behrens, and Doug Hellmann for comments!

.. [#] E-mail me if you think I should write about why :)

.. [#] I can blog about "necessity" vs "sufficiency", too.  Let me know.

.. [#] I've heard estimates of 80-90% of the total cost of development
   for a successful software project, i.e. initial feature development
   is 10-20%, maintenance is 80-90%, but I have no good references for
   this.
